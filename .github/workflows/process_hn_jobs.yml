name: Process Hacker News Jobs

on:
  schedule:
    - cron: '30 0 * * *'
  workflow_dispatch:
  push:
    branches:
      - master

env:
  NODE_VERSION: '20'
  PNPM_VERSION: '8.15.9'

jobs:
  process_jobs:
    name: Process HN Jobs Pipeline
    runs-on: ubuntu-latest
    timeout-minutes: 60

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Setup pnpm
        uses: pnpm/action-setup@v2
        with:
          version: ${{ env.PNPM_VERSION }}

      - name: Get pnpm store directory
        shell: bash
        run: |
          echo "STORE_PATH=$(pnpm store path --silent)" >> $GITHUB_ENV

      - name: Setup pnpm cache
        uses: actions/cache@v4
        with:
          path: ${{ env.STORE_PATH }}
          key: ${{ runner.os }}-pnpm-store-${{ hashFiles('**/pnpm-lock.yaml') }}
          restore-keys: |
            ${{ runner.os }}-pnpm-store-

      - name: Install dependencies
        run: pnpm install --frozen-lockfile

      - name: Fetch latest HN jobs
        id: fetch
        working-directory: apps/registry
        env:
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
        run: |
          echo "::group::Fetching HN jobs"
          node scripts/jobs/import-hn-latest-thread.js 2>&1 | tee fetch.log

          # Extract summary stats
          JOBS_FETCHED=$(grep -oP 'Found \K\d+(?= job postings)' fetch.log || echo "0")
          JOBS_INSERTED=$(grep -oP '‚úÖ \K\d+(?= jobs imported)' fetch.log || echo "0")

          echo "jobs_fetched=${JOBS_FETCHED}" >> $GITHUB_OUTPUT
          echo "jobs_inserted=${JOBS_INSERTED}" >> $GITHUB_OUTPUT
          echo "::endgroup::"
        continue-on-error: false

      - name: AI processing with GPT-5-mini
        id: process
        working-directory: apps/registry
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
        run: |
          echo "::group::AI Processing"

          # Run with timeout and capture output
          timeout 3600 node scripts/jobs/parse-job-descriptions.js 2>&1 | tee process.log

          # Extract processing stats with proper regex patterns
          JOBS_TOTAL=$(grep -oP 'Found \K\d+(?= jobs)' process.log | head -1 || echo "0")
          JOBS_NEED_PROCESSING=$(grep -oP '\K\d+(?= need processing)' process.log | head -1 || echo "0")
          JOBS_PROCESSED=$(grep -oP '‚úÖ Job #\d+' process.log | wc -l | tr -d ' ' || echo "0")
          JOBS_FAILED=$(grep -c '‚ùå Job #' process.log | tr -d ' ' || echo "0")

          # Ensure all values are non-empty
          JOBS_PROCESSED=${JOBS_PROCESSED:-0}
          JOBS_FAILED=${JOBS_FAILED:-0}
          JOBS_NEED_PROCESSING=${JOBS_NEED_PROCESSING:-0}

          echo "jobs_processed=${JOBS_PROCESSED}" >> $GITHUB_OUTPUT
          echo "jobs_failed=${JOBS_FAILED}" >> $GITHUB_OUTPUT
          echo "jobs_total=${JOBS_NEED_PROCESSING}" >> $GITHUB_OUTPUT
          echo "::endgroup::"
        continue-on-error: true

      - name: Vectorize processed jobs
        id: vectorize
        working-directory: apps/registry
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
        run: |
          echo "::group::Vectorization"

          timeout 1800 node scripts/jobs/generate-embeddings-jobs.js 2>&1 | tee vectorize.log

          # Extract vectorization stats
          JOBS_VECTORIZED=$(grep -oP '‚úÖ \K\d+(?= embeddings created)' vectorize.log || echo "0")

          echo "jobs_vectorized=${JOBS_VECTORIZED}" >> $GITHUB_OUTPUT
          echo "::endgroup::"
        continue-on-error: true

      - name: Generate workflow summary
        if: always()
        run: |
          echo "# HN Jobs Processing Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Date:** $(date -u '+%Y-%m-%d %H:%M:%S UTC')" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "## Pipeline Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Stage | Status | Metrics |" >> $GITHUB_STEP_SUMMARY
          echo "|-------|--------|---------|" >> $GITHUB_STEP_SUMMARY

          # Fetch stage
          if [ "${{ steps.fetch.outcome }}" == "success" ]; then
            echo "| üîç Fetch | ‚úÖ Success | Fetched: ${{ steps.fetch.outputs.jobs_fetched }}, Inserted: ${{ steps.fetch.outputs.jobs_inserted }} |" >> $GITHUB_STEP_SUMMARY
          elif [ "${{ steps.fetch.outcome }}" == "skipped" ]; then
            echo "| üîç Fetch | ‚è≠Ô∏è Skipped | User disabled fetch (processing existing jobs only) |" >> $GITHUB_STEP_SUMMARY
          else
            echo "| üîç Fetch | ‚ùå Failed | See logs for details |" >> $GITHUB_STEP_SUMMARY
          fi

          # Process stage
          if [ "${{ steps.process.outcome }}" == "success" ]; then
            echo "| ü§ñ AI Process | ‚úÖ Success | Processed: ${{ steps.process.outputs.jobs_processed }}/${{ steps.process.outputs.jobs_total }}, Failed: ${{ steps.process.outputs.jobs_failed }} |" >> $GITHUB_STEP_SUMMARY
          else
            echo "| ü§ñ AI Process | ‚ùå Failed | Processed: ${{ steps.process.outputs.jobs_processed }}/${{ steps.process.outputs.jobs_total }}, Failed: ${{ steps.process.outputs.jobs_failed }} |" >> $GITHUB_STEP_SUMMARY
          fi

          # Vectorize stage
          if [ "${{ steps.vectorize.outcome }}" == "success" ]; then
            echo "| üî¢ Vectorize | ‚úÖ Success | Vectorized: ${{ steps.vectorize.outputs.jobs_vectorized }} |" >> $GITHUB_STEP_SUMMARY
          else
            echo "| üî¢ Vectorize | ‚ùå Failed | Vectorized: ${{ steps.vectorize.outputs.jobs_vectorized }} |" >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY

          # Add failure notice if any step failed
          if [ "${{ steps.fetch.outcome }}" == "failure" ] || [ "${{ steps.process.outcome }}" == "failure" ] || [ "${{ steps.vectorize.outcome }}" == "failure" ]; then
            echo "## ‚ö†Ô∏è Pipeline Issues Detected" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "One or more stages failed. Check the logs above for details." >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          fi

          echo "## üìä Success Rate" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          TOTAL="${{ steps.process.outputs.jobs_total }}"
          PROCESSED="${{ steps.process.outputs.jobs_processed }}"

          # Ensure variables have default values
          TOTAL=${TOTAL:-0}
          PROCESSED=${PROCESSED:-0}

          if [ "$TOTAL" -gt 0 ] 2>/dev/null; then
            SUCCESS_RATE=$(awk "BEGIN {printf \"%.1f\", ($PROCESSED / $TOTAL) * 100}")
            echo "**AI Processing:** ${SUCCESS_RATE}% (${PROCESSED}/${TOTAL} jobs)" >> $GITHUB_STEP_SUMMARY
          else
            echo "**AI Processing:** No jobs to process" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Upload logs as artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: job-processing-logs-${{ github.run_number }}
          path: |
            apps/registry/fetch.log
            apps/registry/process.log
            apps/registry/vectorize.log
          retention-days: 30

      - name: Notify Discord on completion
        if: always()
        run: |
          # Determine status and emoji
          if [ "${{ job.status }}" == "success" ]; then
            STATUS_EMOJI="‚úÖ"
            STATUS_TEXT="Success"
            COLOR="3066993"
          else
            STATUS_EMOJI="‚ùå"
            STATUS_TEXT="Failed"
            COLOR="15158332"
          fi

          # Get stats with defaults
          JOBS_FETCHED="${{ steps.fetch.outputs.jobs_fetched }}"
          JOBS_INSERTED="${{ steps.fetch.outputs.jobs_inserted }}"
          JOBS_PROCESSED="${{ steps.process.outputs.jobs_processed }}"
          JOBS_TOTAL="${{ steps.process.outputs.jobs_total }}"
          JOBS_FAILED="${{ steps.process.outputs.jobs_failed }}"
          JOBS_VECTORIZED="${{ steps.vectorize.outputs.jobs_vectorized }}"

          # Build the Discord embed
          curl -H "Content-Type: application/json" \
            -X POST \
            -d "{
              \"embeds\": [{
                \"title\": \"${STATUS_EMOJI} HN Jobs Pipeline ${STATUS_TEXT}\",
                \"color\": ${COLOR},
                \"fields\": [
                  {
                    \"name\": \"üîç Fetch\",
                    \"value\": \"Found: ${JOBS_FETCHED:-0}\\nInserted: ${JOBS_INSERTED:-0}\",
                    \"inline\": true
                  },
                  {
                    \"name\": \"ü§ñ AI Processing\",
                    \"value\": \"Processed: ${JOBS_PROCESSED:-0}/${JOBS_TOTAL:-0}\\nFailed: ${JOBS_FAILED:-0}\",
                    \"inline\": true
                  },
                  {
                    \"name\": \"üî¢ Vectorization\",
                    \"value\": \"Embeddings: ${JOBS_VECTORIZED:-0}\",
                    \"inline\": true
                  }
                ],
                \"footer\": {
                  \"text\": \"Run #${{ github.run_number }}\"
                },
                \"timestamp\": \"$(date -u +%Y-%m-%dT%H:%M:%SZ)\",
                \"url\": \"https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}\"
              }]
            }" \
            https://discord.com/api/webhooks/1435731659541446678/p_19MdMzay79WDJ1g5EtfHAC5jAxG93nVMob0bUW4_Igyzl55fb-Uir8z1FpwZPx3bJf

      - name: Create issue on repeated failure
        if: failure() && github.event_name == 'schedule'
        uses: actions/github-script@v7
        with:
          script: |
            // Check if there's already an open issue for pipeline failures
            const issues = await github.rest.issues.listForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              state: 'open',
              labels: 'automated-jobs,bug',
              per_page: 1
            });

            // Only create issue if none exists
            if (issues.data.length === 0) {
              await github.rest.issues.create({
                owner: context.repo.owner,
                repo: context.repo.repo,
                title: 'üö® HN Jobs Processing Pipeline Failure',
                body: `## Pipeline Failure Alert

            The automated HN jobs processing pipeline has failed.

            **Workflow Run:** https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}

            **Date:** ${new Date().toISOString()}

            ### Debugging Steps

            1. Check the workflow logs for error messages
            2. Verify all required secrets are set:
               - \`OPENAI_API_KEY\`
               - \`SUPABASE_KEY\`
               - \`PINECONE_API_KEY\`
               - \`PINECONE_ENVIRONMENT\`
            3. Check API rate limits and quotas
            4. Review recent code changes that may have broken the pipeline

            This issue was automatically created by the GitHub Actions workflow.`,
                labels: ['automated-jobs', 'bug', 'critical']
              });
            }
